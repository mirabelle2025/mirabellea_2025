
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ML Titanic Data | Nighthawk Pages</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="ML Titanic Data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Exploring the Titanic dataset with machine learning." />
<meta property="og:description" content="Exploring the Titanic dataset with machine learning." />
<link rel="canonical" href="http://localhost:4100/mirabellea_2025/csp%20big%20idea%202/2025/02/19/pandas-ml_titanic_IPYNB_2_.html" />
<meta property="og:url" content="http://localhost:4100/mirabellea_2025/csp%20big%20idea%202/2025/02/19/pandas-ml_titanic_IPYNB_2_.html" />
<meta property="og:site_name" content="Nighthawk Pages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-02-19T00:00:00-08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="ML Titanic Data" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-19T00:00:00-08:00","datePublished":"2025-02-19T00:00:00-08:00","description":"Exploring the Titanic dataset with machine learning.","headline":"ML Titanic Data","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4100/mirabellea_2025/csp%20big%20idea%202/2025/02/19/pandas-ml_titanic_IPYNB_2_.html"},"url":"http://localhost:4100/mirabellea_2025/csp%20big%20idea%202/2025/02/19/pandas-ml_titanic_IPYNB_2_.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
  <link rel="stylesheet" href="/mirabellea_2025/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4100/mirabellea_2025/feed.xml" title="Nighthawk Pages" />

<!-- Include favicons --><link rel="shortcut icon" type="image/x-icon" href="/mirabellea_2025/images/favicon.ico"><!-- Include Primer CSS for styling -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />

<!-- Include Font Awesome for icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<!-- Include Hypothesis annotations script if annotations are enabled --><!-- Include KaTeX and MathJax for rendering mathematical expressions if use_math is enabled -->


<script>
/**
 * Function to wrap images with a figure and caption if they have a title attribute.
 * This function is executed when the window loads.
 */
function wrap_img(fn) {
    // Check if the document is already loaded
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        // Select all images within elements with the class 'post'
        var elements = document.querySelectorAll(".post img");
        // Iterate over each image element
        Array.prototype.forEach.call(elements, function(el, i) {
            // Check if the image has a title attribute and is not an emoji
            if (el.getAttribute("title") && (el.className != "emoji")) {
                // Create a figcaption element for the caption
                const caption = document.createElement('figcaption');
                // Create a text node with the title attribute value
                var node = document.createTextNode(el.getAttribute("title"));
                // Append the text node to the figcaption element
                caption.appendChild(node);
                // Create a figure element to wrap the image and caption
                const wrapper = document.createElement('figure');
                // Add the 'image' class to the figure element
                wrapper.className = 'image';
                // Insert the figure element before the image
                el.parentNode.insertBefore(wrapper, el);
                // Remove the image from its original position
                el.parentNode.removeChild(el);
                // Append the image and caption to the figure element
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else {
        // If the document is not loaded, add an event listener to execute the function when it is
        document.addEventListener('DOMContentLoaded', fn);
    }
}
// Execute the wrap_img function when the window loads
window.onload = wrap_img;
</script>

<script>
/**
 * Function to add a link icon to anchor tags with the class 'anchor-link'.
 * This function is executed when the DOM content is loaded.
 */
document.addEventListener("DOMContentLoaded", function(){
    // Select all elements with the class 'anchor-link'
    var elem = document.querySelectorAll(".anchor-link");
    // Iterate over each element and set its inner HTML to a Font Awesome link icon
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
});
</script>
</head>
<body><header class="site-header">

  <div class="wrapper">
    <a class="site-title" rel="author" href="/mirabellea_2025/">Nighthawk Pages</a>
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon fas fa-bars fa-lg"></span>
        </label>

        <div class="drawer-container">
          <div class="drawer">
  <a class="nav-item" href="/mirabellea_2025/blogs/">Blogs</a>
  <a class="nav-item" href="/mirabellea_2025/search/">Search</a>
  <a class="nav-item" href="/mirabellea_2025/about/">About Mirabelle</a>
  <a class="nav-item" href="/mirabellea_2025/README4YML.html">Readme</a>
          </div>
        </div>
        <div class="slab">
  <a class="nav-item" href="/mirabellea_2025/blogs/">Blogs</a>
  <a class="nav-item" href="/mirabellea_2025/search/">Search</a>
  <a class="nav-item" href="/mirabellea_2025/about/">About Mirabelle</a>
  <a class="nav-item" href="/mirabellea_2025/README4YML.html">Readme</a>
        </div>
      </nav>
  </div>
</header>
<main class="page-content" aria-label="Content">
    <div class="wrapper">
      <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header"><h1 class="post-title p-name" itemprop="name headline">ML | Titanic Data</h1><p class="page-description">Exploring the Titanic dataset with machine learning.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2025-02-19T00:00:00-08:00" itemprop="datePublished">
        Feb 19, 2025
      </time>• <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span>
    </p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
          <a class="category-tags-link" href="/mirabellea_2025/categories/cte#CSP Big Idea 2">CSP Big Idea 2</a>
        
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody"><ul><li><a href="#introduction-to-pandas-and-ml">Introduction to Pandas and ML</a><ul><li><a href="#titanic-libraries">Titanic Libraries</a></li><li><a href="#titanic-data">Titanic Data</a></li><li><a href="#clean-titanic-data">Clean Titanic Data</a></li><li><a href="#train-titanic-data">Train Titanic Data</a><ul><li><a href="#median-values">Median Values</a></li><li><a href="#perished-meanaverage">Perished Mean/Average</a></li><li><a href="#survived-meanaverage">Survived Mean/Average</a></li><li><a href="#survived-max-and-min-stats">Survived Max and Min Stats</a></li></ul></li></ul></li><li><a href="#machine-learning">Machine Learning</a><ul><li><a href="#predicting-survival">Predicting Survival</a></li><li><a href="#improve-your-chances">Improve your chances</a></li></ul></li><li><a href="#building-a-python-application">Building a Python Application</a><ul><li><a href="#building-a-backend-model">Building a Backend Model</a></li><li><a href="#building-a-backend-api">Building a Backend API</a></li><li><a href="#register-and-init-in-mainpy">Register and init in main.py</a></li></ul></li></ul><h2 id="introduction-to-pandas-and-ml">Introduction to Pandas and ML</h2>
<blockquote>
  <p>Here is how Mort started on this assignment by asking ChatGPT  … Regarding Python Pandas, what are some data sets that would be good for learning Pandas?</p>
</blockquote>

<ul>
  <li>
    <p>There are many data sets that are suitable for learning pandas, depending on your interests and the skills you want to develop.</p>

    <p>Here are some suggestions…</p>

    <ul>
      <li>
        <p>Titanic Dataset: This is a classic dataset for data analysis and machine learning, and is often used as an introduction to pandas. The dataset contains information about passengers on the Titanic, including their demographics, ticket class, and survival status.</p>
      </li>
      <li>
        <p>Iris Dataset: This is another classic dataset that is often used in machine learning and data analysis courses. The dataset contains information about iris flowers, including their sepal length, sepal width, petal length, and petal width.</p>
      </li>
      <li>
        <p>NBA Player Stats: This dataset contains information about NBA players, including their stats for various games, their positions, and their salaries. It is a great dataset for learning how to clean and manipulate data using pandas.</p>
      </li>
      <li>
        <p>Housing Prices Dataset: This dataset contains information about housing prices in a particular city or region, including variables such as the number of bedrooms, square footage, and price. It is a great dataset for learning how to manipulate and analyze numerical data using pandas.</p>
      </li>
      <li>
        <p>COVID-19 Dataset: This dataset contains information about the COVID-19 pandemic, including the number of cases, deaths, and recoveries in various countries and regions. It is a great dataset for learning how to manipulate and analyze time-series data using pandas.</p>
      </li>
      <li>
        <p>World Bank Dataset: This dataset contains information about economic indicators from various countries, such as GDP, population, and poverty rates. It is a great dataset for learning how to manipulate and analyze large datasets using pandas.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>These are just a few examples of the many datasets that are suitable for learning pandas. The best dataset for you will depend on your interests and the specific skills you want to develop.</p>
  </li>
</ul>

<blockquote>
  <p>Follow up question, “where can I find Titanic data set?”</p>
</blockquote>

<ul>
  <li>
    <p>The Titanic dataset is a popular dataset for data analysis and machine learning, and it can be found on various websites and data repositories. Here are a few places where you can find the Titanic dataset…</p>

    <ul>
      <li>
        <p>Kaggle: The Titanic dataset is available on Kaggle, which is a popular platform for data scientists and machine learning enthusiasts. You can download the dataset from the Kaggle website after creating an account.</p>
      </li>
      <li>
        <p>UCI Machine Learning Repository: The Titanic dataset is also available on the UCI Machine Learning Repository, which is a collection of datasets that are commonly used for machine learning research. You can download the dataset from the UCI Machine Learning Repository website.</p>
      </li>
      <li>
        <p>Seaborn library: If you have the Seaborn library installed in your Python environment, you can load the Titanic dataset directly from the library using the following code:</p>
      </li>
    </ul>
  </li>
</ul>

<h3 id="titanic-libraries">Titanic Libraries</h3>
<p>Using the Titanic dataset will require importing data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
    <span class="n">titanic_data</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'titanic'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Uncomment the following lines to install the required packages
# !pip install seaborn
# !pip install pandas
# !pip install scikit-learn
</span></code></pre></div></div>

<h3 id="titanic-data">Titanic Data</h3>
<blockquote>
  <p>Look at a sample of data.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># Load the titanic dataset
</span><span class="n">titanic_data</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'titanic'</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Titanic Data"</span><span class="p">)</span>


<span class="k">print</span><span class="p">(</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span> <span class="c1"># titanic data set
</span><span class="n">display</span><span class="p">(</span><span class="n">titanic_data</span><span class="p">[[</span><span class="s">'survived'</span><span class="p">,</span><span class="s">'pclass'</span><span class="p">,</span> <span class="s">'sex'</span><span class="p">,</span> <span class="s">'age'</span><span class="p">,</span> <span class="s">'sibsp'</span><span class="p">,</span> <span class="s">'parch'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">,</span> <span class="s">'fare'</span><span class="p">,</span> <span class="s">'embark_town'</span><span class="p">,</span> <span class="s">'alone'</span><span class="p">]])</span> <span class="c1"># look at selected columns
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matplotlib is building the font cache; this may take a moment.


Titanic Data
Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',
       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',
       'alive', 'alone'],
      dtype='object')
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>class</th>
      <th>fare</th>
      <th>embark_town</th>
      <th>alone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>Third</td>
      <td>7.2500</td>
      <td>Southampton</td>
      <td>False</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>First</td>
      <td>71.2833</td>
      <td>Cherbourg</td>
      <td>False</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>Third</td>
      <td>7.9250</td>
      <td>Southampton</td>
      <td>True</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>First</td>
      <td>53.1000</td>
      <td>Southampton</td>
      <td>False</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>Third</td>
      <td>8.0500</td>
      <td>Southampton</td>
      <td>True</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>886</th>
      <td>0</td>
      <td>2</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>Second</td>
      <td>13.0000</td>
      <td>Southampton</td>
      <td>True</td>
    </tr>
    <tr>
      <th>887</th>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>First</td>
      <td>30.0000</td>
      <td>Southampton</td>
      <td>True</td>
    </tr>
    <tr>
      <th>888</th>
      <td>0</td>
      <td>3</td>
      <td>female</td>
      <td>NaN</td>
      <td>1</td>
      <td>2</td>
      <td>Third</td>
      <td>23.4500</td>
      <td>Southampton</td>
      <td>False</td>
    </tr>
    <tr>
      <th>889</th>
      <td>1</td>
      <td>1</td>
      <td>male</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>First</td>
      <td>30.0000</td>
      <td>Cherbourg</td>
      <td>True</td>
    </tr>
    <tr>
      <th>890</th>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>32.0</td>
      <td>0</td>
      <td>0</td>
      <td>Third</td>
      <td>7.7500</td>
      <td>Queenstown</td>
      <td>True</td>
    </tr>
  </tbody>
</table>
<p>891 rows × 10 columns</p>
</div>

<h3 id="clean-titanic-data">Clean Titanic Data</h3>
<p>This is called ‘Cleaning’ data.</p>

<p>Most analysis, like Machine Learning require data to be in standardized format…</p>
<ul>
  <li>All data needs to be numeric</li>
  <li>Some data is removed, as it is not useable in study</li>
  <li>Sex and alone is changed to binary</li>
  <li>The embark data is split into multiple columns</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="c1"># Preprocess the data
</span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="n">td</span> <span class="o">=</span> <span class="n">titanic_data</span>
<span class="n">td</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'alive'</span><span class="p">,</span> <span class="s">'who'</span><span class="p">,</span> <span class="s">'adult_male'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">,</span> <span class="s">'embark_town'</span><span class="p">,</span> <span class="s">'deck'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">td</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># drop rows with at least one missing value, after dropping unuseful columns
</span><span class="n">td</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span> <span class="o">=</span> <span class="n">td</span><span class="p">[</span><span class="s">'sex'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'male'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">td</span><span class="p">[</span><span class="s">'alone'</span><span class="p">]</span> <span class="o">=</span> <span class="n">td</span><span class="p">[</span><span class="s">'alone'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="bp">True</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Encode categorical variables
</span><span class="n">enc</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>
<span class="n">enc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">td</span><span class="p">[[</span><span class="s">'embarked'</span><span class="p">]])</span>
<span class="n">onehot</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">td</span><span class="p">[[</span><span class="s">'embarked'</span><span class="p">]]).</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'embarked_'</span> <span class="o">+</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">enc</span><span class="p">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">td</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">onehot</span><span class="p">)</span>
<span class="n">td</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'embarked'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">td</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># drop rows with at least one missing value, after preparing the data
</span>
<span class="k">print</span><span class="p">(</span><span class="n">td</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">td</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'alone',
       'embarked_C', 'embarked_Q', 'embarked_S'],
      dtype='object')
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>alone</th>
      <th>embarked_C</th>
      <th>embarked_Q</th>
      <th>embarked_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.2500</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>3</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.9250</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1000</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>3</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>8.0500</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>705</th>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>39.0</td>
      <td>0</td>
      <td>0</td>
      <td>26.0000</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>706</th>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>45.0</td>
      <td>0</td>
      <td>0</td>
      <td>13.5000</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>707</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>42.0</td>
      <td>0</td>
      <td>0</td>
      <td>26.2875</td>
      <td>1</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>708</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>22.0</td>
      <td>0</td>
      <td>0</td>
      <td>151.5500</td>
      <td>1</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>710</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>24.0</td>
      <td>0</td>
      <td>0</td>
      <td>49.5042</td>
      <td>1</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
<p>564 rows × 11 columns</p>
</div>

<h3 id="train-titanic-data">Train Titanic Data</h3>
<p>The result of ‘Training’ data is making it easier to analyze or make conclusions.</p>

<p>What conclusions can you make using min, max, means statistics bout the following…</p>
<ul>
  <li>Given that 1-male and 0-femaale, what gender is more likely to suvive?</li>
  <li>Can you make an conclusions on fare?</li>
  <li>Can you make any conclusions on being alone?</li>
</ul>

<h4 id="median-values">Median Values</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">median</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>survived       0.0
pclass         2.0
sex            1.0
age           28.0
sibsp          0.0
parch          0.0
fare          16.1
alone          1.0
embarked_C     0.0
embarked_Q     0.0
embarked_S     1.0
dtype: float64
</code></pre></div></div>

<h4 id="perished-meanaverage">Perished Mean/Average</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"survived == 0"</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>survived       0.000000
pclass         2.464072
sex            0.844311
age           31.073353
sibsp          0.562874
parch          0.398204
fare          24.835902
alone          0.616766
embarked_C     0.185629
embarked_Q     0.038922
embarked_S     0.775449
dtype: float64
</code></pre></div></div>

<h4 id="survived-meanaverage">Survived Mean/Average</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">td</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"survived == 1"</span><span class="p">).</span><span class="n">mean</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>survived       1.000000
pclass         1.878261
sex            0.326087
age           28.481522
sibsp          0.504348
parch          0.508696
fare          50.188806
alone          0.456522
embarked_C     0.152174
embarked_Q     0.034783
embarked_S     0.813043
dtype: float64
</code></pre></div></div>

<h4 id="survived-max-and-min-stats">Survived Max and Min Stats</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">"maximums for survivors"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">td</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"survived == 1"</span><span class="p">).</span><span class="nb">max</span><span class="p">())</span>
<span class="k">print</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s">"minimums for survivors"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">td</span><span class="p">.</span><span class="n">query</span><span class="p">(</span><span class="s">"survived == 1"</span><span class="p">).</span><span class="nb">min</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maximums for survivors
survived        1.0000
pclass          3.0000
sex             1.0000
age            80.0000
sibsp           4.0000
parch           5.0000
fare          512.3292
alone           1.0000
embarked_C      1.0000
embarked_Q      1.0000
embarked_S      1.0000
dtype: float64

minimums for survivors
survived      1.00
pclass        1.00
sex           0.00
age           0.75
sibsp         0.00
parch         0.00
fare          0.00
alone         0.00
embarked_C    0.00
embarked_Q    0.00
embarked_S    0.00
dtype: float64
</code></pre></div></div>

<h2 id="machine-learning">Machine Learning</h2>
<p><a href="https://www.tutorialspoint.com/scikit_learn/scikit_learn_introduction.htm#:~:text=Scikit%2Dlearn%20(Sklearn)%20is,a%20consistence%20interface%20in%20Python">Visit Tutorials Point</a></p>

<blockquote>
  <p>Scikit-learn is a powerful Python library for machine learning, offering tools for classification, regression, clustering, and dimensionality reduction.</p>
</blockquote>

<ul>
  <li>
    <p>The Titanic dataset is a classic for data analysis and machine learning. We’ll use machine learning techniques like Decision Trees and Logistic Regression to predict passenger survival.</p>
  </li>
  <li>
    <p><a href="https://scikit-learn.org/stable/modules/tree.html#tree">Decision Trees</a> are a type of model used for both classification and regression. They work by creating a tree-like model of decisions based on the features. For example, in the context of the Titanic dataset, a Decision Tree might make decisions based on features like ‘age’, ‘sex’, and ‘fare’ to predict whether a passenger survived. The tree might first split by ‘sex’, then for each sex, split by ‘age’, and so on, creating a tree of decisions.</p>
  </li>
  <li>
    <p><a href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">Logistic Regression</a> is a statistical model used in machine learning for binary classification problems. It models the probabilities of the default class (e.g., the probability of a passenger surviving, in the context of the Titanic dataset).</p>
  </li>
  <li>
    <p>After training our models, we’ll evaluate their performance using accuracy, the percentage of correct predictions on unseen data.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Build distinct data frames on survived column
</span><span class="n">X</span> <span class="o">=</span> <span class="n">td</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="s">'survived'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># all except 'survived'
</span><span class="n">y</span> <span class="o">=</span> <span class="n">td</span><span class="p">[</span><span class="s">'survived'</span><span class="p">]</span> <span class="c1"># only 'survived'
</span>
<span class="c1"># Split arrays in random train 70%, random test 30%, using stratified sampling (same proportion of survived in both sets) and a fixed random state (42
# The number 42 is often used in examples and tutorials because of its cultural significance in fields like science fiction (it's the "Answer to the Ultimate Question of Life, The Universe, and Everything" in The Hitchhiker's Guide to the Galaxy by Douglas Adams). But in practice, the actual value doesn't matter; what's important is that it's set to a consistent value.
# X_train is the DataFrame containing the features for the training set.
# X_test is the DataFrame containing the features for the test set.
# y-train is the 'survived' status for each passenger in the training set, corresponding to the X_train data.
# y_test is the 'survived' status for each passenger in the test set, corresponding to the X_test data.
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a decision tree classifier
</span><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Test the model
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">dt</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'DecisionTreeClassifier Accuracy: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>  

<span class="c1"># Train a logistic regression model
</span><span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">logreg</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Test the model
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">logreg</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'LogisticRegression Accuracy: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>  
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecisionTreeClassifier Accuracy: 75.29%
LogisticRegression Accuracy: 78.82%


/Users/johnmortensen/nighthawk/portfolio_2025/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
</code></pre></div></div>

<h3 id="predicting-survival">Predicting Survival</h3>
<p>So, now we are ready to play the game… “Would I have survived the Titanic?”.</p>

<p>Insert your own data in the code.  Look at your analysis and consider how you would travel today.</p>
<ul>
  <li>Data description:
    <ul>
      <li>pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)</li>
      <li>name - Name</li>
      <li>sex - male or female</li>
      <li>age - number of year</li>
      <li>sibsp - number of Siblings/Spouses Aboard</li>
      <li>parch - number of Parents/Children Aboard</li>
      <li>fare - passenger fare 0 to 512</li>
      <li>embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)</li>
      <li>alone - boolean True or False</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Logistic regression model is used to predict the probability
</span>
<span class="c1"># Define a new passenger
</span><span class="n">passenger</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">'name'</span><span class="p">:</span> <span class="p">[</span><span class="s">'John Mortensen'</span><span class="p">],</span>
    <span class="s">'pclass'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="c1"># 2nd class picked as it was median, bargains are my preference, but I don't want to have poor accomodations
</span>    <span class="s">'sex'</span><span class="p">:</span> <span class="p">[</span><span class="s">'male'</span><span class="p">],</span>
    <span class="s">'age'</span><span class="p">:</span> <span class="p">[</span><span class="mi">65</span><span class="p">],</span>
    <span class="s">'sibsp'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="c1"># I usually travel with my wife
</span>    <span class="s">'parch'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="c1"># currenly I have 1 child at home
</span>    <span class="s">'fare'</span><span class="p">:</span> <span class="p">[</span><span class="mf">16.00</span><span class="p">],</span> <span class="c1"># median fare picked assuming it is 2nd class
</span>    <span class="s">'embarked'</span><span class="p">:</span> <span class="p">[</span><span class="s">'S'</span><span class="p">],</span> <span class="c1"># majority of passengers embarked in Southampton
</span>    <span class="s">'alone'</span><span class="p">:</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span> <span class="c1"># travelling with family (spouse and child))
</span><span class="p">})</span>

<span class="n">display</span><span class="p">(</span><span class="n">passenger</span><span class="p">)</span>
<span class="n">new_passenger</span> <span class="o">=</span> <span class="n">passenger</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Preprocess the new passenger data
</span><span class="n">new_passenger</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_passenger</span><span class="p">[</span><span class="s">'sex'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'male'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">new_passenger</span><span class="p">[</span><span class="s">'alone'</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_passenger</span><span class="p">[</span><span class="s">'alone'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="bp">True</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Encode 'embarked' variable
</span><span class="n">onehot</span> <span class="o">=</span> <span class="n">enc</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_passenger</span><span class="p">[[</span><span class="s">'embarked'</span><span class="p">]]).</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'embarked_'</span> <span class="o">+</span> <span class="n">val</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">enc</span><span class="p">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="n">new_passenger</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">onehot</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">new_passenger</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">new_passenger</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'name'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">new_passenger</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'embarked'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">display</span><span class="p">(</span><span class="n">new_passenger</span><span class="p">)</span>

<span class="c1"># Predict the survival probability for the new passenger
</span><span class="n">dead_proba</span><span class="p">,</span> <span class="n">alive_proba</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">logreg</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">new_passenger</span><span class="p">))</span>

<span class="c1"># Print the survival probability
</span><span class="k">print</span><span class="p">(</span><span class="s">'Death probability: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">dead_proba</span><span class="p">))</span>  
<span class="k">print</span><span class="p">(</span><span class="s">'Survival probability: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">alive_proba</span><span class="p">))</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>name</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
      <th>alone</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>John Mortensen</td>
      <td>2</td>
      <td>male</td>
      <td>65</td>
      <td>1</td>
      <td>1</td>
      <td>16.0</td>
      <td>S</td>
      <td>False</td>
    </tr>
  </tbody>
</table>
</div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>alone</th>
      <th>embarked_C</th>
      <th>embarked_Q</th>
      <th>embarked_S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>1</td>
      <td>65</td>
      <td>1</td>
      <td>1</td>
      <td>16.0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Death probability: 90.89%
Survival probability: 9.11%
</code></pre></div></div>

<h3 id="improve-your-chances">Improve your chances</h3>
<p>Is there anything you could do to improve your chances?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Decision tree model is used to determine the importance of each feature
</span>
<span class="n">importances</span> <span class="o">=</span> <span class="n">dt</span><span class="p">.</span><span class="n">feature_importances_</span>
<span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">importance</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">new_passenger</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">importances</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'The importance of </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s"> is: </span><span class="si">{</span><span class="n">importance</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The importance of pclass is: 0.15079115953493002
The importance of sex is: 0.27345943069742495
The importance of age is: 0.23920011105434624
The importance of sibsp is: 0.05411073601351371
The importance of parch is: 0.013914855333419261
The importance of fare is: 0.23936320058241756
The importance of alone is: 0.004181924322029404
The importance of embarked_C is: 0.01202303242583453
The importance of embarked_Q is: 0.0
The importance of embarked_S is: 0.012955550036084264
</code></pre></div></div>

<h2 id="building-a-python-application">Building a Python Application</h2>

<h3 id="building-a-backend-model">Building a Backend Model</h3>
<p>The code above needs to be reformed into a Model.  This Python Class is resturctured to follow Modlel, View (JavaScript), Control (Python API) paradigm.</p>

<p>Be sure to add to the model and “Improve your chances” interface.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Python Titanic Model, prepared for a titanic.py file
</span>
<span class="c1"># Import the required libraries for the TitanicModel class
</span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="k">class</span> <span class="nc">TitanicModel</span><span class="p">:</span>
    <span class="s">"""A class used to represent the Titanic Model for passenger survival prediction.
    """</span>
    <span class="c1"># a singleton instance of TitanicModel, created to train the model only once, while using it for prediction multiple times
</span>    <span class="n">_instance</span> <span class="o">=</span> <span class="bp">None</span>
    
    <span class="c1"># constructor, used to initialize the TitanicModel
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># the titanic ML model
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dt</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># define ML features and target
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">'pclass'</span><span class="p">,</span> <span class="s">'sex'</span><span class="p">,</span> <span class="s">'age'</span><span class="p">,</span> <span class="s">'sibsp'</span><span class="p">,</span> <span class="s">'parch'</span><span class="p">,</span> <span class="s">'fare'</span><span class="p">,</span> <span class="s">'alone'</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">target</span> <span class="o">=</span> <span class="s">'survived'</span>
        <span class="c1"># load the titanic dataset
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s">'titanic'</span><span class="p">)</span>
        <span class="c1"># one-hot encoder used to encode 'embarked' column
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="s">'ignore'</span><span class="p">)</span>

    <span class="c1"># clean the titanic dataset, prepare it for training
</span>    <span class="k">def</span> <span class="nf">_clean</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Drop unnecessary columns
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'alive'</span><span class="p">,</span> <span class="s">'who'</span><span class="p">,</span> <span class="s">'adult_male'</span><span class="p">,</span> <span class="s">'class'</span><span class="p">,</span> <span class="s">'embark_town'</span><span class="p">,</span> <span class="s">'deck'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Convert boolean columns to integers
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="s">'sex'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'male'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="s">'alone'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="s">'alone'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="bp">True</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Drop rows with missing 'embarked' values before one-hot encoding
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'embarked'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># One-hot encode 'embarked' column
</span>        <span class="n">onehot</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[[</span><span class="s">'embarked'</span><span class="p">]]).</span><span class="n">toarray</span><span class="p">()</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'embarked_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">onehot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">onehot</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">,</span> <span class="n">onehot_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'embarked'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Add the one-hot encoded 'embarked' features to the features list
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">features</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="n">cols</span><span class="p">)</span>
        
        <span class="c1"># Drop rows with missing values
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># train the titanic model, using logistic regression as key model, and decision tree to show feature importance
</span>    <span class="k">def</span> <span class="nf">_train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># split the data into features and target
</span>        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">features</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">titanic_data</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">target</span><span class="p">]</span>
        
        <span class="c1"># perform train-test split
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
        
        <span class="c1"># train the model
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
        <span class="c1"># train a decision tree classifier
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dt</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        
    <span class="o">@</span><span class="nb">classmethod</span>
    <span class="k">def</span> <span class="nf">get_instance</span><span class="p">(</span><span class="n">cls</span><span class="p">):</span>
        <span class="s">""" Gets, and conditionaly cleans and builds, the singleton instance of the TitanicModel.
        The model is used for analysis on titanic data and predictions on the survival of theoritical passengers.
        
        Returns:
            TitanicModel: the singleton _instance of the TitanicModel, which contains data and methods for prediction.
        """</span>        
        <span class="c1"># check for instance, if it doesn't exist, create it
</span>        <span class="k">if</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span> <span class="o">=</span> <span class="n">cls</span><span class="p">()</span>
            <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span><span class="p">.</span><span class="n">_clean</span><span class="p">()</span>
            <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span><span class="p">.</span><span class="n">_train</span><span class="p">()</span>
        <span class="c1"># return the instance, to be used for prediction
</span>        <span class="k">return</span> <span class="n">cls</span><span class="p">.</span><span class="n">_instance</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">passenger</span><span class="p">):</span>
        <span class="s">""" Predict the survival probability of a passenger.

        Args:
            passenger (dict): A dictionary representing a passenger. The dictionary should contain the following keys:
                'pclass': The passenger's class (1, 2, or 3)
                'sex': The passenger's sex ('male' or 'female')
                'age': The passenger's age
                'sibsp': The number of siblings/spouses the passenger has aboard
                'parch': The number of parents/children the passenger has aboard
                'fare': The fare the passenger paid
                'embarked': The port at which the passenger embarked ('C', 'Q', or 'S')
                'alone': Whether the passenger is alone (True or False)

        Returns:
           dictionary : contains die and survive probabilities 
        """</span>
        <span class="c1"># clean the passenger data
</span>        <span class="n">passenger_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">passenger</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">passenger_df</span><span class="p">[</span><span class="s">'sex'</span><span class="p">]</span> <span class="o">=</span> <span class="n">passenger_df</span><span class="p">[</span><span class="s">'sex'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s">'male'</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">passenger_df</span><span class="p">[</span><span class="s">'alone'</span><span class="p">]</span> <span class="o">=</span> <span class="n">passenger_df</span><span class="p">[</span><span class="s">'alone'</span><span class="p">].</span><span class="nb">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="bp">True</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">onehot</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">passenger_df</span><span class="p">[[</span><span class="s">'embarked'</span><span class="p">]]).</span><span class="n">toarray</span><span class="p">()</span>
        <span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'embarked_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">.</span><span class="n">categories_</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">onehot_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">onehot</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">cols</span><span class="p">)</span>
        <span class="n">passenger_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">passenger_df</span><span class="p">,</span> <span class="n">onehot_df</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">passenger_df</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'embarked'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># predict the survival probability and extract the probabilities from numpy array
</span>        <span class="n">die</span><span class="p">,</span> <span class="n">survive</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">passenger_df</span><span class="p">))</span>
        <span class="c1"># return the survival probabilities as a dictionary
</span>        <span class="k">return</span> <span class="p">{</span><span class="s">'die'</span><span class="p">:</span> <span class="n">die</span><span class="p">,</span> <span class="s">'survive'</span><span class="p">:</span> <span class="n">survive</span><span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">feature_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"""Get the feature weights
        The weights represent the relative importance of each feature in the prediction model.

        Returns:
            dictionary: contains each feature as a key and its weight of importance as a value
        """</span>
        <span class="c1"># extract the feature importances from the decision tree model
</span>        <span class="n">importances</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">dt</span><span class="p">.</span><span class="n">feature_importances_</span>
        <span class="c1"># return the feature importances as a dictionary, using dictionary comprehension
</span>        <span class="k">return</span> <span class="p">{</span><span class="n">feature</span><span class="p">:</span> <span class="n">importance</span> <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">importance</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">features</span><span class="p">,</span> <span class="n">importances</span><span class="p">)}</span> 
    
<span class="k">def</span> <span class="nf">initTitanic</span><span class="p">():</span>
    <span class="s">""" Initialize the Titanic Model.
    This function is used to load the Titanic Model into memory, and prepare it for prediction.
    """</span>
    <span class="n">TitanicModel</span><span class="p">.</span><span class="n">get_instance</span><span class="p">()</span>
    
<span class="k">def</span> <span class="nf">testTitanic</span><span class="p">():</span>
    <span class="s">""" Test the Titanic Model
    Using the TitanicModel class, we can predict the survival probability of a passenger.
    Print output of this test contains method documentation, passenger data, survival probability, and survival weights.
    """</span>
     
    <span class="c1"># setup passenger data for prediction
</span>    <span class="k">print</span><span class="p">(</span><span class="s">" Step 1:  Define theoritical passenger data for prediction: "</span><span class="p">)</span>
    <span class="n">passenger</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">'name'</span><span class="p">:</span> <span class="p">[</span><span class="s">'John Mortensen'</span><span class="p">],</span>
        <span class="s">'pclass'</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="s">'sex'</span><span class="p">:</span> <span class="p">[</span><span class="s">'male'</span><span class="p">],</span>
        <span class="s">'age'</span><span class="p">:</span> <span class="p">[</span><span class="mi">65</span><span class="p">],</span>
        <span class="s">'sibsp'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="s">'parch'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="s">'fare'</span><span class="p">:</span> <span class="p">[</span><span class="mf">16.00</span><span class="p">],</span>
        <span class="s">'embarked'</span><span class="p">:</span> <span class="p">[</span><span class="s">'S'</span><span class="p">],</span>
        <span class="s">'alone'</span><span class="p">:</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">,</span> <span class="n">passenger</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>

    <span class="c1"># get an instance of the cleaned and trained Titanic Model
</span>    <span class="n">titanicModel</span> <span class="o">=</span> <span class="n">TitanicModel</span><span class="p">.</span><span class="n">get_instance</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">" Step 2:"</span><span class="p">,</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">get_instance</span><span class="p">.</span><span class="n">__doc__</span><span class="p">)</span>
   
    <span class="c1"># print the survival probability
</span>    <span class="k">print</span><span class="p">(</span><span class="s">" Step 3:"</span><span class="p">,</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">predict</span><span class="p">.</span><span class="n">__doc__</span><span class="p">)</span>
    <span class="n">probability</span> <span class="o">=</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">passenger</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s"> death probability: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">probability</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'die'</span><span class="p">)))</span>  
    <span class="k">print</span><span class="p">(</span><span class="s">'</span><span class="se">\t</span><span class="s"> survival probability: {:.2%}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">probability</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'survive'</span><span class="p">)))</span>
    <span class="k">print</span><span class="p">()</span>
    
    <span class="c1"># print the feature weights in the prediction model
</span>    <span class="k">print</span><span class="p">(</span><span class="s">" Step 4:"</span><span class="p">,</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">feature_weights</span><span class="p">.</span><span class="n">__doc__</span><span class="p">)</span>
    <span class="n">importances</span> <span class="o">=</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">feature_weights</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">importance</span> <span class="ow">in</span> <span class="n">importances</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\t\t</span><span class="s">"</span><span class="p">,</span> <span class="n">feature</span><span class="p">,</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">importance</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="o">%</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> <span class="c1"># importance of each feature, each key/value pair
</span>        
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="s">" Begin:"</span><span class="p">,</span> <span class="n">testTitanic</span><span class="p">.</span><span class="n">__doc__</span><span class="p">)</span>
    <span class="n">testTitanic</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> Begin:  Test the Titanic Model
    Using the TitanicModel class, we can predict the survival probability of a passenger.
    Print output of this test contains method documentation, passenger data, survival probability, and survival weights.
    
 Step 1:  Define theoritical passenger data for prediction: 
	 {'name': ['John Mortensen'], 'pclass': [2], 'sex': ['male'], 'age': [65], 'sibsp': [1], 'parch': [1], 'fare': [16.0], 'embarked': ['S'], 'alone': [False]}

 Step 2:  Gets, and conditionaly cleans and builds, the singleton instance of the TitanicModel.
        The model is used for analysis on titanic data and predictions on the survival of theoritical passengers.
        
        Returns:
            TitanicModel: the singleton _instance of the TitanicModel, which contains data and methods for prediction.
        
 Step 3:  Predict the survival probability of a passenger.

        Args:
            passenger (dict): A dictionary representing a passenger. The dictionary should contain the following keys:
                'pclass': The passenger's class (1, 2, or 3)
                'sex': The passenger's sex ('male' or 'female')
                'age': The passenger's age
                'sibsp': The number of siblings/spouses the passenger has aboard
                'parch': The number of parents/children the passenger has aboard
                'fare': The fare the passenger paid
                'embarked': The port at which the passenger embarked ('C', 'Q', or 'S')
                'alone': Whether the passenger is alone (True or False)

        Returns:
           dictionary : contains die and survive probabilities 
        
	 death probability: 93.46%
	 survival probability: 6.54%

 Step 4: Get the feature weights
        The weights represent the relative importance of each feature in the prediction model.

        Returns:
            dictionary: contains each feature as a key and its weight of importance as a value
        
		 pclass 11.87%
		 sex 29.65%
		 age 24.34%
		 sibsp 5.96%
		 parch 2.03%
		 fare 22.13%
		 alone 0.68%
		 embarked_C 0.43%
		 embarked_Q 0.79%
		 embarked_S 2.13%
</code></pre></div></div>

<h3 id="building-a-backend-api">Building a Backend API</h3>
<p>API code is then created to work with the Model.   This example is in imperitive style.  This is unlike the object style we use in user.py.   Teacher suggestion is to adapt this to style you are using in your code base.</p>

<p>Be sure to resturcture this code to work well with you Web applications frontend.  The dictionary keys you pass into the TitanicModel.predict() must match the requirements shown in the docstring comments of the predict method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Python Titanic Sample API endpoint
</span><span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Blueprint</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">jsonify</span>
<span class="kn">from</span> <span class="nn">flask_restful</span> <span class="kn">import</span> <span class="n">Api</span><span class="p">,</span> <span class="n">Resource</span> <span class="c1"># used for REST API building
</span>
<span class="c1"># Import the TitanicModel class from the model file
# from model.titanic import TitanicModel
</span>
<span class="n">titanic_api</span> <span class="o">=</span> <span class="n">Blueprint</span><span class="p">(</span><span class="s">'titanic_api'</span><span class="p">,</span> <span class="n">__name__</span><span class="p">,</span>
                   <span class="n">url_prefix</span><span class="o">=</span><span class="s">'/api/titanic'</span><span class="p">)</span>

<span class="n">api</span> <span class="o">=</span> <span class="n">Api</span><span class="p">(</span><span class="n">titanic_api</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TitanicAPI</span><span class="p">:</span>
    <span class="k">class</span> <span class="nc">_Predict</span><span class="p">(</span><span class="n">Resource</span><span class="p">):</span>
        
        <span class="k">def</span> <span class="nf">post</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="s">""" Semantics: In HTTP, POST requests are used to send data to the server for processing.
            Sending passenger data to the server to get a prediction fits the semantics of a POST request.
            
            POST requests send data in the body of the request...
            1. which can handle much larger amounts of data and data types, than URL parameters
            2. using an HTTPS request, the data is encrypted, making it more secure
            3. a JSON formated body is easy to read and write between JavaScript and Python, great for Postman testing
            """</span>     
            <span class="c1"># Get the passenger data from the request
</span>            <span class="n">passenger</span> <span class="o">=</span> <span class="n">request</span><span class="p">.</span><span class="n">get_json</span><span class="p">()</span>

            <span class="c1"># Get the singleton instance of the TitanicModel
</span>            <span class="n">titanicModel</span> <span class="o">=</span> <span class="n">TitanicModel</span><span class="p">.</span><span class="n">get_instance</span><span class="p">()</span>
            <span class="c1"># Predict the survival probability of the passenger
</span>            <span class="n">response</span> <span class="o">=</span> <span class="n">titanicModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">passenger</span><span class="p">)</span>

            <span class="c1"># Return the response as JSON
</span>            <span class="k">return</span> <span class="n">jsonify</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="n">api</span><span class="p">.</span><span class="n">add_resource</span><span class="p">(</span><span class="n">_Predict</span><span class="p">,</span> <span class="s">'/predict'</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---------------------------------------------------------------------------

ModuleNotFoundError                       Traceback (most recent call last)

Cell In[12], line 2
      1 ## Python Titanic Sample API endpoint
----&gt; 2 from flask import Blueprint, request, jsonify
      3 from flask_restful import Api, Resource # used for REST API building
      5 # Import the TitanicModel class from the model file
      6 # from model.titanic import TitanicModel


ModuleNotFoundError: No module named 'flask'
</code></pre></div></div>

<h3 id="register-and-init-in-mainpy">Register and init in main.py</h3>
<p>As with all API’s it will be necessary to register your APIs.  This is following same pattern as importing and then registering the blueprint.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">app</span><span class="p">.</span><span class="n">register_blueprint</span><span class="p">(</span><span class="n">titanic_api</span><span class="p">)</span> <span class="c1"># register api routes
</span></code></pre></div></div>

<p>Additionally, the initTitanic method was made in the model to be included with generate_data commands.  This makes sure that the Titanic model is loaded at startup, avoiding cleaning and training delays for each prediction.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">custom_cli</span><span class="p">.</span><span class="n">command</span><span class="p">(</span><span class="s">'generate_data'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">generate_data</span><span class="p">():</span>
    <span class="n">initUsers</span><span class="p">()</span>
    <span class="n">initPlayers</span><span class="p">()</span>
    <span class="n">initTitanic</span><span class="p">()</span> <span class="c1"># init titanic data
</span></code></pre></div></div>

  </div><a class="u-url" href="/mirabellea_2025/csp%20big%20idea%202/2025/02/19/pandas-ml_titanic_IPYNB_2_.html" hidden></a>
</article>

    </div>
  </main><footer class="site-footer h-card">
  <data class="u-url" href="/mirabellea_2025/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4100/mirabellea_2025/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
              />
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Class of 2025</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>
</html>
